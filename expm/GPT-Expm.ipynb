{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "**Source** : https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./data/gpt-input.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open(local_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the corpus in characters:  1115393\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the corpus in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# unique characters that occur in this corpus\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest Character-Level Encoding/Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 6, 1, 20, 53, 61, 1, 39, 56, 43, 1, 63, 53, 59, 12]\n",
      "Hi, How are you?\n"
     ]
    }
   ],
   "source": [
    "# mapping from characters to integers and  vice versa\n",
    "ctoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itoc = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# encoder: take a string, output a list of integers\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "\n",
    "# decoder: take a list of integers, output a string\n",
    "decode = lambda n: ''.join([itoc[i] for i in n])\n",
    "\n",
    "greeting = \"Hi, How are you?\"\n",
    "print(encode(greeting))\n",
    "print(decode(encode(greeting)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '$',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '3',\n",
       " 10: ':',\n",
       " 11: ';',\n",
       " 12: '?',\n",
       " 13: 'A',\n",
       " 14: 'B',\n",
       " 15: 'C',\n",
       " 16: 'D',\n",
       " 17: 'E',\n",
       " 18: 'F',\n",
       " 19: 'G',\n",
       " 20: 'H',\n",
       " 21: 'I',\n",
       " 22: 'J',\n",
       " 23: 'K',\n",
       " 24: 'L',\n",
       " 25: 'M',\n",
       " 26: 'N',\n",
       " 27: 'O',\n",
       " 28: 'P',\n",
       " 29: 'Q',\n",
       " 30: 'R',\n",
       " 31: 'S',\n",
       " 32: 'T',\n",
       " 33: 'U',\n",
       " 34: 'V',\n",
       " 35: 'W',\n",
       " 36: 'X',\n",
       " 37: 'Y',\n",
       " 38: 'Z',\n",
       " 39: 'a',\n",
       " 40: 'b',\n",
       " 41: 'c',\n",
       " 42: 'd',\n",
       " 43: 'e',\n",
       " 44: 'f',\n",
       " 45: 'g',\n",
       " 46: 'h',\n",
       " 47: 'i',\n",
       " 48: 'j',\n",
       " 49: 'k',\n",
       " 50: 'l',\n",
       " 51: 'm',\n",
       " 52: 'n',\n",
       " 53: 'o',\n",
       " 54: 'p',\n",
       " 55: 'q',\n",
       " 56: 'r',\n",
       " 57: 's',\n",
       " 58: 't',\n",
       " 59: 'u',\n",
       " 60: 'v',\n",
       " 61: 'w',\n",
       " 62: 'x',\n",
       " 63: 'y',\n",
       " 64: 'z'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece\n",
    "by **Google** \\\n",
    "https://github.com/google/sentencepiece\n",
    "\n",
    "Imagine you have a magical tool that can slice words into smaller, bite-sized pieces, unlocking a world of efficiency and flexibility in language processing. This tool, known as SentencePiece, doesn't just deal with whole words or individual charactersâ€”it's like a linguistic chef, breaking down words into flavorful subword morsels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TikToken\n",
    "by **OpenAI** \\\n",
    "https://platform.openai.com/tokenizer \\\n",
    "https://github.com/openai/tiktoken\n",
    "\n",
    "TikToken acts as your linguistic compass, swiftly breaking down the complex tapestry of language into bite-sized tokens, much like a skilled artisan carefully carving intricate sculptures from a block of marble. With the finesse of a master craftsman, TikToken employs the power of Byte Pair Encoding (BPE) to dissect text into its elemental units, allowing OpenAI's models to navigate the linguistic landscape with unparalleled precision and efficiency.\n",
    "\n",
    "Picture TikToken as a conductor orchestrating a symphony of words, seamlessly segmenting sentences into meaningful fragments, each note harmonizing with the next to create a melodious composition of language. Its speed is unmatched, akin to a nimble dancer gracefully gliding across the stage, effortlessly tokenizing text at lightning speed, ensuring that the flow of information remains uninterrupted and fluid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a training and validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 90% will be training data, rest validation data\n",
    "n = int(0.9*len(data)) \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Not going to feed the entire corpus to the transformer. Instead, provide random chunks sampled from anywhere in the corpus. Let's define the size of the chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_size = 8\n",
    "train_data[:context_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:context_size]\n",
    "y = train_data[1:context_size+1]\n",
    "for t in range(context_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]    \n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 53, 59, 56,  1, 43, 52, 43],\n",
      "        [ 1, 44, 39, 56, 43, 57,  1, 53],\n",
      "        [ 1, 51, 43,  0, 32, 53,  1, 46],\n",
      "        [27, 10,  0, 13, 57,  1, 52, 43]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 59, 56,  1, 43, 52, 43, 51],\n",
      "        [44, 39, 56, 43, 57,  1, 53, 59],\n",
      "        [51, 43,  0, 32, 53,  1, 46, 43],\n",
      "        [10,  0, 13, 57,  1, 52, 43, 39]])\n",
      "----\n",
      "when input is [1] the target: 53\n",
      "when input is [1, 53] the target: 59\n",
      "when input is [1, 53, 59] the target: 56\n",
      "when input is [1, 53, 59, 56] the target: 1\n",
      "when input is [1, 53, 59, 56, 1] the target: 43\n",
      "when input is [1, 53, 59, 56, 1, 43] the target: 52\n",
      "when input is [1, 53, 59, 56, 1, 43, 52] the target: 43\n",
      "when input is [1, 53, 59, 56, 1, 43, 52, 43] the target: 51\n",
      "when input is [1] the target: 44\n",
      "when input is [1, 44] the target: 39\n",
      "when input is [1, 44, 39] the target: 56\n",
      "when input is [1, 44, 39, 56] the target: 43\n",
      "when input is [1, 44, 39, 56, 43] the target: 57\n",
      "when input is [1, 44, 39, 56, 43, 57] the target: 1\n",
      "when input is [1, 44, 39, 56, 43, 57, 1] the target: 53\n",
      "when input is [1, 44, 39, 56, 43, 57, 1, 53] the target: 59\n",
      "when input is [1] the target: 51\n",
      "when input is [1, 51] the target: 43\n",
      "when input is [1, 51, 43] the target: 0\n",
      "when input is [1, 51, 43, 0] the target: 32\n",
      "when input is [1, 51, 43, 0, 32] the target: 53\n",
      "when input is [1, 51, 43, 0, 32, 53] the target: 1\n",
      "when input is [1, 51, 43, 0, 32, 53, 1] the target: 46\n",
      "when input is [1, 51, 43, 0, 32, 53, 1, 46] the target: 43\n",
      "when input is [27] the target: 10\n",
      "when input is [27, 10] the target: 0\n",
      "when input is [27, 10, 0] the target: 13\n",
      "when input is [27, 10, 0, 13] the target: 57\n",
      "when input is [27, 10, 0, 13, 57] the target: 1\n",
      "when input is [27, 10, 0, 13, 57, 1] the target: 52\n",
      "when input is [27, 10, 0, 13, 57, 1, 52] the target: 43\n",
      "when input is [27, 10, 0, 13, 57, 1, 52, 43] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1344)\n",
    "\n",
    "batch_size = 4 # number of independent sequence to proceed in parallel\n",
    "context_size = 8 # maximum context length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # random sampling\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "# batch\n",
    "for b in range(batch_size): \n",
    "    # time\n",
    "    for t in range(context_size): \n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 53, 59, 56,  1, 43, 52, 43],\n",
      "        [ 1, 44, 39, 56, 43, 57,  1, 53],\n",
      "        [ 1, 51, 43,  0, 32, 53,  1, 46],\n",
      "        [27, 10,  0, 13, 57,  1, 52, 43]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NLP_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
